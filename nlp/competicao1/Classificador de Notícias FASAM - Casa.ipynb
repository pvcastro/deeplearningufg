{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pt_core_news_sm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path('../dataset/functions_classifier')\n",
    "# train_data_path = data_path / 'train.jsonl'\n",
    "# validation_data_path = data_path /'dev.jsonl'\n",
    "# test_data_path = data_path /'test.jsonl'\n",
    "validation_data_path = 'df_valid.jsonl'\n",
    "train_data_path = 'df_train.jsonl'\n",
    "validation_title_data_path = 'df_valid_title.jsonl'\n",
    "train_title_data_path = 'df_train_title.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(train_data_path, orient='records', lines=True)\n",
    "dev_df = pd.read_json(validation_data_path, orient='records', lines=True)\n",
    "train_title_df = pd.read_json(train_title_data_path, orient='records', lines=True)\n",
    "dev_title_df = pd.read_json(validation_title_data_path, orient='records', lines=True)\n",
    "#test_df = pd.read_json(test_data_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7894, 94917)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = count_vect.fit_transform(train_df['text'])\n",
    "train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7894, 94917)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935949618446822"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', max_iter=1000, tol=1e-4, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train_df['text'], train_df['label'])\n",
    "predicted_svm = text_clf_svm.predict(train_df['text'])\n",
    "balanced_accuracy_score(train_df['label'], predicted_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], \n",
    "              'tfidf__use_idf': (True, False), \n",
    "              'tfidf__norm': ('l1', 'l2'), \n",
    "              'clf-svm__alpha': (1e-2, 1e-3), \n",
    "              'clf-svm__penalty': ('none', 'l2', 'l1', 'elasticnet')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 64 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 77.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed: 111.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829313450804446\n",
      "{'clf-svm__alpha': 0.001, 'clf-svm__penalty': 'none', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf_svm, parameters, scoring='balanced_accuracy', n_jobs=-1, cv=10, iid=True, verbose=True)\n",
    "gs_clf = gs_clf.fit(train_df['text'], train_df['label'])\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4251"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gs_clf.predict(dev_df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(predictions, out_path):\n",
    "    count = 0\n",
    "\n",
    "    with open(out_path, mode='w', encoding='utf-8') as out_file:\n",
    "        print('Saving predictions for %s' % validation_data_path)\n",
    "        out_file.write('id,category\\n')\n",
    "        idx = 0\n",
    "        for result in predictions:\n",
    "            count += 1\n",
    "            out_file.write(str(idx) + ',' + result + '\\n')\n",
    "            idx += 1\n",
    "            if count % 100 == 0:\n",
    "                print('Predicted %d sentences' % count)\n",
    "    out_file.close()\n",
    "    print('Finished predicting %d sentences' % count)\n",
    "    print('Results saved in %s' % Path(out_path).absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for df_valid.jsonl\n",
      "Predicted 100 sentences\n",
      "Predicted 200 sentences\n",
      "Predicted 300 sentences\n",
      "Predicted 400 sentences\n",
      "Predicted 500 sentences\n",
      "Predicted 600 sentences\n",
      "Predicted 700 sentences\n",
      "Predicted 800 sentences\n",
      "Predicted 900 sentences\n",
      "Predicted 1000 sentences\n",
      "Predicted 1100 sentences\n",
      "Predicted 1200 sentences\n",
      "Predicted 1300 sentences\n",
      "Predicted 1400 sentences\n",
      "Predicted 1500 sentences\n",
      "Predicted 1600 sentences\n",
      "Predicted 1700 sentences\n",
      "Predicted 1800 sentences\n",
      "Predicted 1900 sentences\n",
      "Predicted 2000 sentences\n",
      "Predicted 2100 sentences\n",
      "Predicted 2200 sentences\n",
      "Predicted 2300 sentences\n",
      "Predicted 2400 sentences\n",
      "Predicted 2500 sentences\n",
      "Predicted 2600 sentences\n",
      "Predicted 2700 sentences\n",
      "Predicted 2800 sentences\n",
      "Predicted 2900 sentences\n",
      "Predicted 3000 sentences\n",
      "Predicted 3100 sentences\n",
      "Predicted 3200 sentences\n",
      "Predicted 3300 sentences\n",
      "Predicted 3400 sentences\n",
      "Predicted 3500 sentences\n",
      "Predicted 3600 sentences\n",
      "Predicted 3700 sentences\n",
      "Predicted 3800 sentences\n",
      "Predicted 3900 sentences\n",
      "Predicted 4000 sentences\n",
      "Predicted 4100 sentences\n",
      "Predicted 4200 sentences\n",
      "Finished predicting 4251 sentences\n",
      "Results saved in /home/user/Documentos/Mestrado/FASAM/submissions_text_clf_svm.csv\n"
     ]
    }
   ],
   "source": [
    "write_predictions(text_clf_svm.predict(dev_df['text']), 'submissions_text_clf_svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for df_valid.jsonl\n",
      "Predicted 100 sentences\n",
      "Predicted 200 sentences\n",
      "Predicted 300 sentences\n",
      "Predicted 400 sentences\n",
      "Predicted 500 sentences\n",
      "Predicted 600 sentences\n",
      "Predicted 700 sentences\n",
      "Predicted 800 sentences\n",
      "Predicted 900 sentences\n",
      "Predicted 1000 sentences\n",
      "Predicted 1100 sentences\n",
      "Predicted 1200 sentences\n",
      "Predicted 1300 sentences\n",
      "Predicted 1400 sentences\n",
      "Predicted 1500 sentences\n",
      "Predicted 1600 sentences\n",
      "Predicted 1700 sentences\n",
      "Predicted 1800 sentences\n",
      "Predicted 1900 sentences\n",
      "Predicted 2000 sentences\n",
      "Predicted 2100 sentences\n",
      "Predicted 2200 sentences\n",
      "Predicted 2300 sentences\n",
      "Predicted 2400 sentences\n",
      "Predicted 2500 sentences\n",
      "Predicted 2600 sentences\n",
      "Predicted 2700 sentences\n",
      "Predicted 2800 sentences\n",
      "Predicted 2900 sentences\n",
      "Predicted 3000 sentences\n",
      "Predicted 3100 sentences\n",
      "Predicted 3200 sentences\n",
      "Predicted 3300 sentences\n",
      "Predicted 3400 sentences\n",
      "Predicted 3500 sentences\n",
      "Predicted 3600 sentences\n",
      "Predicted 3700 sentences\n",
      "Predicted 3800 sentences\n",
      "Predicted 3900 sentences\n",
      "Predicted 4000 sentences\n",
      "Predicted 4100 sentences\n",
      "Predicted 4200 sentences\n",
      "Finished predicting 4251 sentences\n",
      "Results saved in /home/user/Documentos/Mestrado/FASAM/submissions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "write_predictions(gs_clf.predict(dev_df['text']), 'submissions_svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentsIterator(object):\n",
    "    def __init__(self, dataframe, has_labels=True):\n",
    "        \"\"\"\n",
    "        Um generator de documentos que produz os tokens filtrados a partir dos textos dos documentos.\n",
    "        :param documents: Lista ou iterator de documentos.\n",
    "        :param text_field: Nome do campo a partir do qual será extraído o texto dos documentos.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.has_labels = has_labels\n",
    "        self.nlp = pt_core_news_sm.load()\n",
    "        self.nlp.remove_pipe('parser')\n",
    "        self.nlp.remove_pipe('ner')\n",
    "        self.stopwords = [line.replace('\\n', '') for line in open('stopwords-pt.txt', mode='r', encoding='utf8').readlines()]\n",
    "\n",
    "    def get_relevant_tokens(self):\n",
    "        \"\"\"\n",
    "        Tipos dos Part of Speech tags definidas pelo spacy para os tokens produzidos:\n",
    "        ADJ (Adjetivo)\n",
    "        ADP (Adposição - Preposições e Posposições) -> Desprezar\n",
    "        ADV (Advérbio) -> Desprezar\n",
    "        AUX (Auxiliar) -> Desprezar\n",
    "        CCONJ (Conjunção coordenativa) -> Desprezar\n",
    "        DET (Determinante - Artigos, Pronomes numerais, etc) -> Desprezar\n",
    "        INTJ (Interjeição) -> Desprezar\n",
    "        NOUN (Substantivo)\n",
    "        NUM (Numeral) -> Desprezar\n",
    "        PART (Partícula) -> Desprezar\n",
    "        PRON (Pronome) -> Desprezar\n",
    "        PROPN (Nome próprio)\n",
    "        PUNCT (Pontuação) -> Desprezar\n",
    "        SCONJ (Conjunção subordinativa) -> Desprezar\n",
    "        SYM (Símbolo) -> Desprezar\n",
    "        VERB (Verbo)\n",
    "        X (Outros) -> Desprezar\n",
    "        :return: Generator de lemmas dos tokens, convertidos para lowercase, filtrados de acordo com as POS tags e\n",
    "        tamanho.\n",
    "        \"\"\"\n",
    "        print('pre-processing the documents')\n",
    "        if self.has_labels:\n",
    "            labels = self.dataframe['label']\n",
    "        texts = self.dataframe['text']\n",
    "        docs = self.nlp.pipe(self.dataframe['text'], n_threads=4, batch_size=100)\n",
    "        for index, doc in enumerate(docs):\n",
    "            doc = [token.lemma_.lower() for token in doc if token.is_alpha and not token.text.lower() in self.stopwords]\n",
    "            if self.has_labels:\n",
    "                label = labels[index] if self.has_labels else None\n",
    "                # Recupera os lemmas para lowercase dos tokens que não forem stopwords\n",
    "                yield {'label': label, 'text': doc}\n",
    "            else:\n",
    "                yield {'text': doc}\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_relevant_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_df(dataframe, name_json, has_labels=True):\n",
    "    iterator = DocumentsIterator(dataframe, has_labels)\n",
    "    count = 0\n",
    "    texts = []\n",
    "    if has_labels:\n",
    "        labels = []\n",
    "    for doc in iterator:\n",
    "        if has_labels:\n",
    "            labels.append(doc['label'])\n",
    "        texts.append(' '.join(doc['text']))\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('Processed %d documents' % count)\n",
    "    if has_labels:\n",
    "        processed_df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "    else:\n",
    "        processed_df = pd.DataFrame({'text': texts})\n",
    "    processed_df.to_json(name_json, orient='records', lines=True)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing the documents\n",
      "Processed 500 documents\n",
      "Processed 1000 documents\n",
      "Processed 1500 documents\n",
      "Processed 2000 documents\n",
      "Processed 2500 documents\n",
      "Processed 3000 documents\n",
      "Processed 3500 documents\n",
      "Processed 4000 documents\n",
      "Processed 4500 documents\n",
      "Processed 5000 documents\n",
      "Processed 5500 documents\n",
      "Processed 6000 documents\n",
      "Processed 6500 documents\n",
      "Processed 7000 documents\n",
      "Processed 7500 documents\n"
     ]
    }
   ],
   "source": [
    "train_processed_df = get_processed_df(train_df, 'df_train_lemma_no_stop.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comida</td>\n",
       "      <td>casa barra funda clima roceiro receitar saboro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>educacao</td>\n",
       "      <td>professores sp decidir manter greve fechar pis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>edição concurso pago pequeno empresar prêmio f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>equilibrioesaude</td>\n",
       "      <td>maconha saudar físico estudar esforçar entende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ciencia</td>\n",
       "      <td>percorrer km revelar cientista roto migração i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>líderes inovador reunem redar compartilhar ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>turismo</td>\n",
       "      <td>conheça pandora atração milionário disney insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>fiesp organizar edição maratona hacker atenção...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>turismo</td>\n",
       "      <td>praia forte misturar natureza resorts estrutur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turismo</td>\n",
       "      <td>app agência avisar vestir passaporte vencer ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>turismo</td>\n",
       "      <td>app caronas viagem temer comparação uber criad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sobretudo</td>\n",
       "      <td>tipo veículo precisar manter extintor bordar p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ciencia</td>\n",
       "      <td>rica fóssil dino uberaba geoparque estimular t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tec</td>\n",
       "      <td>yahoo prejuízo milhão trimestre potencial bala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>bazar beneficente comido típico sírio feito fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>turismo</td>\n",
       "      <td>lugar voar balão brasil possibilidade voar bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tec</td>\n",
       "      <td>loja aplicativo google oferecer opção brasilei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>turismo</td>\n",
       "      <td>paris proibir circulação carro trecho longo ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tec</td>\n",
       "      <td>chinesa zte lucrar dobrar passar fabricante ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comida</td>\n",
       "      <td>gwyneth paltrow desistir desafiar passar dia c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>educacao</td>\n",
       "      <td>tema redação enem vazar início provar polícia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>educacao</td>\n",
       "      <td>planejamento evitar apertar orçamentar pagar p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tec</td>\n",
       "      <td>mensagem gifs valer palavra lucy dikeou aluno ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>ong arrecadar material escolar criança escola ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>educacao</td>\n",
       "      <td>educação médio conseguir formar quadro técnico...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sobretudo</td>\n",
       "      <td>artista plástico ensinar suporte vaso tirar ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tec</td>\n",
       "      <td>confira kit decoração unhar tetris bugiganga t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>ciclo brilhante inspirar brasileiro abrir negó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ambiente</td>\n",
       "      <td>trump mentir aberto acordar paris presidente e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>educacao</td>\n",
       "      <td>temida químico exigir relação cotidiano saber ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>ciencia</td>\n",
       "      <td>arriscar manter higiene ninhar norte auratus d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>tec</td>\n",
       "      <td>programa deixar windows caro windows lançado w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>turismo</td>\n",
       "      <td>brasileiros jeito baratear ir eua saber econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>turismo</td>\n",
       "      <td>conhecido ilhar santa lúcia paisagem tirar fôl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>sobretudo</td>\n",
       "      <td>criado eua conceito mba tropicalizado servir q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7869</th>\n",
       "      <td>sobretudo</td>\n",
       "      <td>precisar abrir mão espaçar localização buscar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870</th>\n",
       "      <td>equilibrioesaude</td>\n",
       "      <td>azeitar fritar regrar culinário derrubar ciênc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7871</th>\n",
       "      <td>tec</td>\n",
       "      <td>navegador celular positivo negativo celular co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7872</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>fgv eaesp lançar mestrado profissional sustent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7873</th>\n",
       "      <td>educacao</td>\n",
       "      <td>justiça realizar audiência conciliação estudan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>empreendedores social cidadania focar negócio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7875</th>\n",
       "      <td>comida</td>\n",
       "      <td>mudança clima levar marcar francês produzir ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7876</th>\n",
       "      <td>tec</td>\n",
       "      <td>aventura político zelândia afeta reputação rei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>ambiente</td>\n",
       "      <td>imagem satélite mostrar movimentar iceberg gig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7878</th>\n",
       "      <td>sobretudo</td>\n",
       "      <td>saiba limpar lavador roupar corretar paulo joa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7879</th>\n",
       "      <td>equilibrioesaude</td>\n",
       "      <td>ansiedade matemático pai contagiar filho defic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7880</th>\n",
       "      <td>educacao</td>\n",
       "      <td>mec receber sugestão mudança provar enem suges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>tec</td>\n",
       "      <td>apple conquistar patente aparelhar realidade v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7882</th>\n",
       "      <td>comida</td>\n",
       "      <td>veg curitiba festival tipo coxa vegana n sábad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>turismo</td>\n",
       "      <td>comido companhia aéreo servir avião presidente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>amo lixão afirmar catador trocar casar aprende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>equilibrioesaude</td>\n",
       "      <td>bolhar gengiva dever preocupar bolhar gengiva ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7886</th>\n",
       "      <td>ciencia</td>\n",
       "      <td>planetas solar batizados concurso astronomia p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7887</th>\n",
       "      <td>equilibrioesaude</td>\n",
       "      <td>entrada zika brasil ser copa confederações doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>comida</td>\n",
       "      <td>veg comidas vegetariano bicicleta perdão sumiç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>ciencia</td>\n",
       "      <td>mensageiro sideral olhar n planeta astrônomos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7890</th>\n",
       "      <td>educacao</td>\n",
       "      <td>verba mec suspender pronatec continuidade prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>educacao</td>\n",
       "      <td>deputado pt agredir pm invasão assembleia víde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>educacao</td>\n",
       "      <td>governo avaliar reduzir descontar automático s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7893</th>\n",
       "      <td>empreendedorsocial</td>\n",
       "      <td>calouro virar ceo redar escola inglês custar p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label                                               text\n",
       "0                 comida  casa barra funda clima roceiro receitar saboro...\n",
       "1               educacao  professores sp decidir manter greve fechar pis...\n",
       "2     empreendedorsocial  edição concurso pago pequeno empresar prêmio f...\n",
       "3       equilibrioesaude  maconha saudar físico estudar esforçar entende...\n",
       "4                ciencia  percorrer km revelar cientista roto migração i...\n",
       "5     empreendedorsocial  líderes inovador reunem redar compartilhar ide...\n",
       "6                turismo  conheça pandora atração milionário disney insp...\n",
       "7     empreendedorsocial  fiesp organizar edição maratona hacker atenção...\n",
       "8                turismo  praia forte misturar natureza resorts estrutur...\n",
       "9                turismo  app agência avisar vestir passaporte vencer ap...\n",
       "10               turismo  app caronas viagem temer comparação uber criad...\n",
       "11             sobretudo  tipo veículo precisar manter extintor bordar p...\n",
       "12               ciencia  rica fóssil dino uberaba geoparque estimular t...\n",
       "13                   tec  yahoo prejuízo milhão trimestre potencial bala...\n",
       "14    empreendedorsocial  bazar beneficente comido típico sírio feito fa...\n",
       "15               turismo  lugar voar balão brasil possibilidade voar bal...\n",
       "16                   tec  loja aplicativo google oferecer opção brasilei...\n",
       "17               turismo  paris proibir circulação carro trecho longo ri...\n",
       "18                   tec  chinesa zte lucrar dobrar passar fabricante ch...\n",
       "19                comida  gwyneth paltrow desistir desafiar passar dia c...\n",
       "20              educacao  tema redação enem vazar início provar polícia ...\n",
       "21              educacao  planejamento evitar apertar orçamentar pagar p...\n",
       "22                   tec  mensagem gifs valer palavra lucy dikeou aluno ...\n",
       "23    empreendedorsocial  ong arrecadar material escolar criança escola ...\n",
       "24              educacao  educação médio conseguir formar quadro técnico...\n",
       "25             sobretudo  artista plástico ensinar suporte vaso tirar ca...\n",
       "26                   tec  confira kit decoração unhar tetris bugiganga t...\n",
       "27    empreendedorsocial  ciclo brilhante inspirar brasileiro abrir negó...\n",
       "28              ambiente  trump mentir aberto acordar paris presidente e...\n",
       "29              educacao  temida químico exigir relação cotidiano saber ...\n",
       "...                  ...                                                ...\n",
       "7864             ciencia  arriscar manter higiene ninhar norte auratus d...\n",
       "7865                 tec  programa deixar windows caro windows lançado w...\n",
       "7866             turismo  brasileiros jeito baratear ir eua saber econom...\n",
       "7867             turismo  conhecido ilhar santa lúcia paisagem tirar fôl...\n",
       "7868           sobretudo  criado eua conceito mba tropicalizado servir q...\n",
       "7869           sobretudo  precisar abrir mão espaçar localização buscar ...\n",
       "7870    equilibrioesaude  azeitar fritar regrar culinário derrubar ciênc...\n",
       "7871                 tec  navegador celular positivo negativo celular co...\n",
       "7872  empreendedorsocial  fgv eaesp lançar mestrado profissional sustent...\n",
       "7873            educacao  justiça realizar audiência conciliação estudan...\n",
       "7874  empreendedorsocial  empreendedores social cidadania focar negócio ...\n",
       "7875              comida  mudança clima levar marcar francês produzir ch...\n",
       "7876                 tec  aventura político zelândia afeta reputação rei...\n",
       "7877            ambiente  imagem satélite mostrar movimentar iceberg gig...\n",
       "7878           sobretudo  saiba limpar lavador roupar corretar paulo joa...\n",
       "7879    equilibrioesaude  ansiedade matemático pai contagiar filho defic...\n",
       "7880            educacao  mec receber sugestão mudança provar enem suges...\n",
       "7881                 tec  apple conquistar patente aparelhar realidade v...\n",
       "7882              comida  veg curitiba festival tipo coxa vegana n sábad...\n",
       "7883             turismo  comido companhia aéreo servir avião presidente...\n",
       "7884  empreendedorsocial  amo lixão afirmar catador trocar casar aprende...\n",
       "7885    equilibrioesaude  bolhar gengiva dever preocupar bolhar gengiva ...\n",
       "7886             ciencia  planetas solar batizados concurso astronomia p...\n",
       "7887    equilibrioesaude  entrada zika brasil ser copa confederações doe...\n",
       "7888              comida  veg comidas vegetariano bicicleta perdão sumiç...\n",
       "7889             ciencia  mensageiro sideral olhar n planeta astrônomos ...\n",
       "7890            educacao  verba mec suspender pronatec continuidade prog...\n",
       "7891            educacao  deputado pt agredir pm invasão assembleia víde...\n",
       "7892            educacao  governo avaliar reduzir descontar automático s...\n",
       "7893  empreendedorsocial  calouro virar ceo redar escola inglês custar p...\n",
       "\n",
       "[7894 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed_df = pd.read_json('df_train_lemma_no_stop.jsonl', orient='records', lines=True)\n",
    "train_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing the documents\n",
      "Processed 500 documents\n",
      "Processed 1000 documents\n",
      "Processed 1500 documents\n",
      "Processed 2000 documents\n",
      "Processed 2500 documents\n",
      "Processed 3000 documents\n",
      "Processed 3500 documents\n",
      "Processed 4000 documents\n"
     ]
    }
   ],
   "source": [
    "valid_processed_df = get_processed_df(dev_df, 'df_valid_lemma_no_stop.jsonl', has_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vitrine dilma pronatec orçamentar compassar es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>direito autoral publicidade youtubers reclamar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rótulos alimento alertar lactose decidir anvis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sociedade britânico compositor processar sound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fies aluno madrugar portar fmu conseguir datar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cientistas aguardar nascimento raro dragão cav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aplicativo mostrar voar turismo preparar seleç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pesquisas indicar sono direção perigoso álcool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brasil lançar missão lua estudar vidar espaçar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cientistas testar vacinar colesterol doença ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>farol torre eiffel manutenção começar operar d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>finalistas empreendedor social mentoria artemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eventos marcar comemoração aniversário sp prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>folha realizar fórum saudar recessão sp dia ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>masterchef bater recorde twitter vencedor temp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>complexo gastronômico inaugurar recife conferi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>empresa presidente twitter square entrar bolsa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lojas restaurante testemunhar transformação ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fotógrafo registrar cânions gaúcho preto branc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fórum discutir inclusão teatral focar projeto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>artista criar infernar favela consigo beleza m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>casal fotografar cena paisagem mundo lembrar m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fóssil mandíbula levar descobrir bichar parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>food truck mocotó vender dadinhos tapioca buta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cientistas comparar onda cerebral algoritmo ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>música antigo criar computador restaurar divul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lua júpiter virar aposto cientista buscar vida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>projeto transformar multiúso tirar tv sala pau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>especialistas estimar tri transição economia v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>reservar mineração igual desmate procuradoria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>fenômeno el niño forte história especialistas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>mensageiro sideral treinamento jornalismo cien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>pirituba concentrar condomínio gigantesco cheg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>carreira direito ganhar fama interessar operaç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>reconhecimento causar reconhecer wildlife cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>refazer provar antigo enem ajudar reta profess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>negócios social voltar infância lacuna mercar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>carros chegar valorizar espaçar internar lista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4229</th>\n",
       "      <td>proposta acabar despachar gratuito bagagem voo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>cai escola ocupar paulo escola ocupar reorgani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>brasileiro eleito melhor jogador videogame mun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>microsoft executivo mudar jeito gigante ver we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>artemisia seleciona negócio impactar programar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>reclamação linkedin diminuir enviar principal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>fumar invernar mal saudar fumante notar invern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>consumo cereal integrar aumentar longevidade a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>metrô vila guilherme crescer marginal tietê do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>microsoft abandonar marcar internet explorer m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>oms comemorar avanço lutar doença negligenciar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>modelo financiamento estudantil debater brasil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>queda cotação euro estimular comprar moeda tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>governo prometer pagar professorar descontar g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>astrônomos usp tirar dinheiro bolsar pagar pes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>venda pcs despencar trimestre consultoria come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>azul mar encontrar paisagem desertar aruba car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>barbie gravar converso virar espião alemanha o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>crescimento serviço nuvem evitar quedo receita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>revolucionário projeto interestelar apoiar ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>chef suíço suicidar ser vítima fraudar finance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>humoristas satirizar comportamento brasileiro ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4251 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     vitrine dilma pronatec orçamentar compassar es...\n",
       "1     direito autoral publicidade youtubers reclamar...\n",
       "2     rótulos alimento alertar lactose decidir anvis...\n",
       "3     sociedade britânico compositor processar sound...\n",
       "4     fies aluno madrugar portar fmu conseguir datar...\n",
       "5     cientistas aguardar nascimento raro dragão cav...\n",
       "6     aplicativo mostrar voar turismo preparar seleç...\n",
       "7     pesquisas indicar sono direção perigoso álcool...\n",
       "8     brasil lançar missão lua estudar vidar espaçar...\n",
       "9     cientistas testar vacinar colesterol doença ca...\n",
       "10    farol torre eiffel manutenção começar operar d...\n",
       "11    finalistas empreendedor social mentoria artemi...\n",
       "12    eventos marcar comemoração aniversário sp prog...\n",
       "13    folha realizar fórum saudar recessão sp dia ju...\n",
       "14    masterchef bater recorde twitter vencedor temp...\n",
       "15    complexo gastronômico inaugurar recife conferi...\n",
       "16    empresa presidente twitter square entrar bolsa...\n",
       "17    lojas restaurante testemunhar transformação ru...\n",
       "18    fotógrafo registrar cânions gaúcho preto branc...\n",
       "19    fórum discutir inclusão teatral focar projeto ...\n",
       "20    artista criar infernar favela consigo beleza m...\n",
       "21    casal fotografar cena paisagem mundo lembrar m...\n",
       "22    fóssil mandíbula levar descobrir bichar parent...\n",
       "23    food truck mocotó vender dadinhos tapioca buta...\n",
       "24    cientistas comparar onda cerebral algoritmo ci...\n",
       "25    música antigo criar computador restaurar divul...\n",
       "26    lua júpiter virar aposto cientista buscar vida...\n",
       "27    projeto transformar multiúso tirar tv sala pau...\n",
       "28    especialistas estimar tri transição economia v...\n",
       "29    reservar mineração igual desmate procuradoria ...\n",
       "...                                                 ...\n",
       "4221  fenômeno el niño forte história especialistas ...\n",
       "4222  mensageiro sideral treinamento jornalismo cien...\n",
       "4223  pirituba concentrar condomínio gigantesco cheg...\n",
       "4224  carreira direito ganhar fama interessar operaç...\n",
       "4225  reconhecimento causar reconhecer wildlife cons...\n",
       "4226  refazer provar antigo enem ajudar reta profess...\n",
       "4227  negócios social voltar infância lacuna mercar ...\n",
       "4228  carros chegar valorizar espaçar internar lista...\n",
       "4229  proposta acabar despachar gratuito bagagem voo...\n",
       "4230  cai escola ocupar paulo escola ocupar reorgani...\n",
       "4231  brasileiro eleito melhor jogador videogame mun...\n",
       "4232  microsoft executivo mudar jeito gigante ver we...\n",
       "4233  artemisia seleciona negócio impactar programar...\n",
       "4234  reclamação linkedin diminuir enviar principal ...\n",
       "4235  fumar invernar mal saudar fumante notar invern...\n",
       "4236  consumo cereal integrar aumentar longevidade a...\n",
       "4237  metrô vila guilherme crescer marginal tietê do...\n",
       "4238  microsoft abandonar marcar internet explorer m...\n",
       "4239  oms comemorar avanço lutar doença negligenciar...\n",
       "4240  modelo financiamento estudantil debater brasil...\n",
       "4241  queda cotação euro estimular comprar moeda tur...\n",
       "4242  governo prometer pagar professorar descontar g...\n",
       "4243  astrônomos usp tirar dinheiro bolsar pagar pes...\n",
       "4244  venda pcs despencar trimestre consultoria come...\n",
       "4245  azul mar encontrar paisagem desertar aruba car...\n",
       "4246  barbie gravar converso virar espião alemanha o...\n",
       "4247  crescimento serviço nuvem evitar quedo receita...\n",
       "4248  revolucionário projeto interestelar apoiar ste...\n",
       "4249  chef suíço suicidar ser vítima fraudar finance...\n",
       "4250  humoristas satirizar comportamento brasileiro ...\n",
       "\n",
       "[4251 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_processed_df = pd.read_json('df_valid_lemma_no_stop.jsonl', orient='records', lines=True)\n",
    "valid_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993320976411534"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_clf_svm = Pipeline([('vect', CountVectorizer(stop_words=stopwords)), ('tfidf', TfidfTransformer()),\n",
    "                              ('clf-svm', SGDClassifier(loss='hinge', max_iter=2000, tol=1e-5, random_state=42))])\n",
    "\n",
    "processed_clf_svm = processed_clf_svm.fit(train_df['text'], train_df['label'])\n",
    "predicted_svm = processed_clf_svm.predict(train_df['text'])\n",
    "balanced_accuracy_score(train_df['label'], predicted_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for df_valid.jsonl\n",
      "Predicted 100 sentences\n",
      "Predicted 200 sentences\n",
      "Predicted 300 sentences\n",
      "Predicted 400 sentences\n",
      "Predicted 500 sentences\n",
      "Predicted 600 sentences\n",
      "Predicted 700 sentences\n",
      "Predicted 800 sentences\n",
      "Predicted 900 sentences\n",
      "Predicted 1000 sentences\n",
      "Predicted 1100 sentences\n",
      "Predicted 1200 sentences\n",
      "Predicted 1300 sentences\n",
      "Predicted 1400 sentences\n",
      "Predicted 1500 sentences\n",
      "Predicted 1600 sentences\n",
      "Predicted 1700 sentences\n",
      "Predicted 1800 sentences\n",
      "Predicted 1900 sentences\n",
      "Predicted 2000 sentences\n",
      "Predicted 2100 sentences\n",
      "Predicted 2200 sentences\n",
      "Predicted 2300 sentences\n",
      "Predicted 2400 sentences\n",
      "Predicted 2500 sentences\n",
      "Predicted 2600 sentences\n",
      "Predicted 2700 sentences\n",
      "Predicted 2800 sentences\n",
      "Predicted 2900 sentences\n",
      "Predicted 3000 sentences\n",
      "Predicted 3100 sentences\n",
      "Predicted 3200 sentences\n",
      "Predicted 3300 sentences\n",
      "Predicted 3400 sentences\n",
      "Predicted 3500 sentences\n",
      "Predicted 3600 sentences\n",
      "Predicted 3700 sentences\n",
      "Predicted 3800 sentences\n",
      "Predicted 3900 sentences\n",
      "Predicted 4000 sentences\n",
      "Predicted 4100 sentences\n",
      "Predicted 4200 sentences\n",
      "Finished predicting 4251 sentences\n",
      "Results saved in /home/user/Documentos/Mestrado/FASAM/submissions_stop_clf_svm.csv\n"
     ]
    }
   ],
   "source": [
    "write_predictions(processed_clf_svm.predict(dev_df['text']), 'submissions_stop_clf_svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '/media/discoD/models/scikit-learn/functions/judge_classifier.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 96.4640 %\n"
     ]
    }
   ],
   "source": [
    "# Save to file in the current working directory\n",
    "with open(model_path, 'wb') as file:  \n",
    "    pickle.dump(gs_clf, file)\n",
    "\n",
    "# Load from file\n",
    "with open(model_path, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)\n",
    "\n",
    "# Calculate the accuracy score and predict target values\n",
    "prediction = pickle_model.predict(test_df['text'])\n",
    "print(\"Test score: {0:.4f} %\".format(100 * balanced_accuracy_score(test_df['label'], prediction)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_model.predict(['Servidor Responsável'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_model.predict(['Assistente de Juiz', 'Assistente do Juiz', 'Juiz Assistente', 'Juiz Substituto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
