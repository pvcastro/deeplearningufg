{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'taxa_aprendizagem'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60c8126ec63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m               [0.6, 0.5, 0.55]]),\n\u001b[1;32m    124\u001b[0m     \u001b[1;31m# taxa de aprendizagem e saidas desejáveis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mtaxa_aprendizagem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesejados\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[0mnna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_entradas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-60c8126ec63f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, neuronios_por_camada, hidden_weigths, saida_weigths, taxa_aprendizagem, desejados)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_weigths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNeuronio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"H\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtaxa_aprendizagem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaida_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'taxa_aprendizagem'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(u):\n",
    "    return 1 / (1 + math.exp(-u))\n",
    "\n",
    "\n",
    "class Neuronio(object):\n",
    "\n",
    "    def __init__(self, indice, camada, pesos, taxa_aprendizagem, desejado=0.0):\n",
    "        self.indice = indice\n",
    "        self.camada = camada\n",
    "        self.pesos = pesos\n",
    "        self.desejado = desejado\n",
    "        self.taxa_aprendizagem = taxa_aprendizagem\n",
    "        self.entradas = []\n",
    "        self.saida = 0\n",
    "        self.erro = 0\n",
    "\n",
    "    def set_entradas(self, entradas):\n",
    "        ##Adiciona um coeficiente 1 para o bias na matriz de entradas\n",
    "        self.entradas = np.append(1, entradas)\n",
    "\n",
    "    def propagate(self):\n",
    "        \"\"\" Propaga as entradas pelo neuronio (somatório ponderado -> sigmoide) \"\"\"\n",
    "        summation = np.sum(self.entradas*self.pesos)\n",
    "        self.saida = sigmoid(summation)\n",
    "        print(self.name, \":\", self.saida)\n",
    "        return self.saida\n",
    "\n",
    "    def calculate_e_out(self):\n",
    "        \"\"\" Calcula a derivada do erro total pela saida do neuronio da camada saida \"\"\"\n",
    "        return -(self.desejado - self.saida)\n",
    "\n",
    "    def calculate_e_out_net(self, e_out):\n",
    "        \"\"\" Calcula a derivada da saida do neuronio pelo somatório \"\"\"\n",
    "        out_net = self.saida*(1-self.saida)\n",
    "        return e_out*out_net\n",
    "\n",
    "    def retro_propagate(self, e_out):\n",
    "        \"\"\" Recebe e_out (derivada do erro total pela saida do neuronio em questão e recalcula os pesos \"\"\"\n",
    "        e_out_net = self.calculate_e_out_net(e_out)\n",
    "        self.pesos = self.pesos-(self.entradas*e_out_net*self.taxa_aprendizagem)\n",
    "        print(\"New Weights\", self.name, self.pesos)\n",
    "\n",
    "    def calculate_erro(self):\n",
    "        \"\"\" Calcula o erro quadrático e retorna \"\"\"\n",
    "        self.erro = self.desejado-self.saida\n",
    "        self.erro = (self.erro*self.erro)/2\n",
    "        print(\"Error\", self.name, self.erro)\n",
    "        return self.erro\n",
    "\n",
    "\n",
    "class MultilayerPerceptron(object):\n",
    "    \"\"\" Classe responsável por representar uma rede neural Multilayer Perceptron com duas camadas \"\"\"\n",
    "\n",
    "    def __init__(self, neuronios_por_camada, hidden_weigths, saida_weigths, taxa_aprendizagem, desejados):\n",
    "        self.camadas = []\n",
    "        for camada, neuronios in enumerate(neuronios_por_camada):\n",
    "            for indice in range(neuronios):\n",
    "                \"\"\" Se for a última camada, adiciona os valores desejados \"\"\"\n",
    "                if camada == len(neuronios_por_camada) - 1:\n",
    "                    neuronio = Neuronio(indice=indice, camada=camada, pesos=[], taxa_aprendizagem=taxa_aprendizagem)\n",
    "                else:\n",
    "                    neuronio = Neuronio(indice=indice, camada=camada, pesos=[], taxa_aprendizagem=taxa_aprendizagem)\n",
    "        self.hidden_layer = []\n",
    "        for k, w in enumerate(hidden_weigths):\n",
    "            self.hidden_layer.append(Neuronio(\"H\"+str(k), np.array(w), taxa_aprendizagem))\n",
    "\n",
    "        self.saida_layer = []\n",
    "        for k, w in enumerate(saida_weigths):\n",
    "            self.saida_layer.append(Neuronio(\"O\"+str(k), np.array(w), taxa_aprendizagem, desejados[k]))\n",
    "\n",
    "    def set_entradas(self, entradas):\n",
    "        for h in self.hidden_layer:\n",
    "            h.set_entradas(np.array(entradas))\n",
    "\n",
    "    def propagate(self):\n",
    "        saidas = []\n",
    "        # propaga na camada hidden\n",
    "        for h in self.hidden_layer:\n",
    "            saidas.append(h.propagate())\n",
    "        # pega as saidas da camada hidden, joga como entrada na camada saida e propaga nessa camada\n",
    "        for o in self.saida_layer:\n",
    "            o.set_entradas(np.array(saidas))\n",
    "            o.propagate()\n",
    "\n",
    "    def retro_propagate(self):\n",
    "        # e_out_h é a derivada do Erro no neuronio Oi em ghj (saida do neuronio j da camada hidden)\n",
    "        e_out_h = []\n",
    "        for o in self.saida_layer:\n",
    "            e_out = o.calculate_e_out()\n",
    "            e_out_net = o.calculate_e_out_net(e_out)\n",
    "            e_out_h.append(o.pesos * e_out_net)\n",
    "            # retro propaga na camada de saida\n",
    "            o.retro_propagate(e_out)\n",
    "\n",
    "        # faz a transposta visto que para cada neuronio da camada de entrada é necessário o somatorio das derivadas de\n",
    "        # acordo com erros de cada neuronio de saida\n",
    "        e_out_h = np.transpose(e_out_h)\n",
    "\n",
    "        for k, h in enumerate(self.hidden_layer):\n",
    "            # faz o somatório das derivadas de todos os erros em ghj e propaga na camada hidden\n",
    "            h.retro_propagate(np.sum(e_out_h[k]))\n",
    "\n",
    "    def calculate_erro(self):\n",
    "        \"\"\" Calcula o erro somando os erros quadráticos de cada neuronio da camada de saida \"\"\"\n",
    "        erro = 0\n",
    "        for o in self.saida_layer:\n",
    "            erro += o.calculate_erro()\n",
    "        return erro\n",
    "\n",
    "\n",
    "nna = MultilayerPerceptron(\n",
    "    neuronios_por_camada=[2,2],\n",
    "    # Pesos e bias da camada hidden\n",
    "    hidden_weigths=np.array([[0.35, 0.15, 0.2], \n",
    "              [0.35, 0.25, 0.3]]),\n",
    "    # Pesos e bias da camada saida\n",
    "    saida_weigths=np.array([[0.6, 0.4, 0.45], \n",
    "              [0.6, 0.5, 0.55]]),\n",
    "    # taxa de aprendizagem e saidas desejáveis\n",
    "    taxa_aprendizagem=0.5, desejados=np.array([0.01, 0.99])\n",
    ")\n",
    "nna.set_entradas(np.array([0.05, 0.1]))\n",
    "\n",
    "for i in range(0, 500):\n",
    "    print(\"========== Epoca \"+str(i+1)+\" ==========\")\n",
    "    nna.propagate()\n",
    "    nna.retro_propagate()\n",
    "    plot.plot(i+1, nna.calculate_erro(), marker='o')\n",
    "    print(\"Total Error:\", nna.calculate_erro())\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}